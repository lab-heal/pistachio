{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5dda9b",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "-   weekly performance, only show best model, weeks 0 to 20\n",
    "-   run variations of prediction window (lookahead) + frequency, plot of sensitivity by frequency/lookahead\n",
    "-   sensitivity + specificity + AUC+ROC\n",
    "-   do weekly versions + trendline, not cum\n",
    "-   for manuscript -- use terms e.g., \"scheduled retraining\", \"closed loop\"\n",
    "-   why even do online learning -- capture \"in situ\" context, don't simply apply historical model to new cohort\n",
    "-   experiment with oversampling recent data (or sample weighting, focus on the positive observations)\n",
    "    -   look into regret minimization algos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e89cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/rygbfpzx3l7fcpbc7540q88w0000gr/T/ipykernel_27476/904874444.py:9: DtypeWarning: Columns (93,122,123,124,125,126) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import engineer_features, prep_X_y\n",
    "\n",
    "DATA_DIR = Path(\"./pistachio_1_data\")\n",
    "dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhu.henian/projects/pistachio/util.py:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(df[\"ActivityDateTime\"]) - pd.to_datetime(df[\"Therapy Start\"])\n"
     ]
    }
   ],
   "source": [
    "sorted_dyads_df = dyads_df.sort_values(\n",
    "    by=\"ActivityDateTime\", key=lambda x: pd.to_datetime(x)\n",
    ")\n",
    "cleaned_dyads_df = engineer_features(\n",
    "    sorted_dyads_df,\n",
    "    stress_lookback_days=0,\n",
    ")\n",
    "\n",
    "\n",
    "df_train = cleaned_dyads_df[cleaned_dyads_df[\"Arm_Sham\"]]\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=False)\n",
    "df_test = cleaned_dyads_df[~cleaned_dyads_df[\"Arm_Sham\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbf717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class HrModel(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        mean_hr = X[\"hr_moving_avg_10m\"]\n",
    "        return (mean_hr > 80) & (mean_hr < 129)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        preds = self.predict(X)\n",
    "        proba = np.zeros((len(X), 2))\n",
    "        proba[:, 1] = preds.astype(float)\n",
    "        proba[:, 0] = 1 - proba[:, 1]\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b635154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhu.henian/projects/pistachio/util.py:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(df[\"ActivityDateTime\"]) - pd.to_datetime(df[\"Therapy Start\"])\n"
     ]
    }
   ],
   "source": [
    "cleaned_dyads_df = engineer_features(dyads_df)\n",
    "\n",
    "df_train = cleaned_dyads_df[cleaned_dyads_df[\"Arm_Sham\"]]\n",
    "df_test = cleaned_dyads_df[~cleaned_dyads_df[\"Arm_Sham\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344e2bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 12-19 16:57:55] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 12-19 16:57:55] {1763} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 12-19 16:57:55] {1862} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl.logger: 12-19 16:57:55] {1979} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 0, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2417} INFO - Estimated sufficient time budget=5941s. Estimated necessary time budget=6s.\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.0252,\tbest estimator xgboost's best error=0.0252\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0252,\tbest estimator xgboost's best error=0.0252\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0252,\tbest estimator xgboost's best error=0.0252\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0248,\tbest estimator xgboost's best error=0.0248\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0248,\tbest estimator xgboost's best error=0.0248\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.7s,\testimator xgboost's best error=0.0248,\tbest estimator xgboost's best error=0.0248\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 6, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.7s,\testimator xgboost's best error=0.0248,\tbest estimator xgboost's best error=0.0248\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.7s,\testimator xgboost's best error=0.0248,\tbest estimator xgboost's best error=0.0248\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.8s,\testimator xgboost's best error=0.0235,\tbest estimator xgboost's best error=0.0235\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.8s,\testimator xgboost's best error=0.0231,\tbest estimator xgboost's best error=0.0231\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 1.9s,\testimator xgboost's best error=0.0231,\tbest estimator xgboost's best error=0.0231\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 2.0s,\testimator xgboost's best error=0.0231,\tbest estimator xgboost's best error=0.0231\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 2.0s,\testimator xgboost's best error=0.0223,\tbest estimator xgboost's best error=0.0223\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2466} INFO -  at 2.1s,\testimator xgboost's best error=0.0223,\tbest estimator xgboost's best error=0.0223\n",
      "[flaml.automl.logger: 12-19 16:57:55] {2282} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.2s,\testimator xgboost's best error=0.0223,\tbest estimator xgboost's best error=0.0223\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.2s,\testimator xgboost's best error=0.0223,\tbest estimator xgboost's best error=0.0223\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.4s,\testimator xgboost's best error=0.0220,\tbest estimator xgboost's best error=0.0220\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.4s,\testimator xgboost's best error=0.0220,\tbest estimator xgboost's best error=0.0220\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.9s,\testimator xgboost's best error=0.0210,\tbest estimator xgboost's best error=0.0210\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2466} INFO -  at 2.9s,\testimator xgboost's best error=0.0210,\tbest estimator xgboost's best error=0.0210\n",
      "[flaml.automl.logger: 12-19 16:57:56] {2282} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:57] {2466} INFO -  at 3.9s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:57:57] {2282} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:58] {2466} INFO -  at 4.5s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:57:58] {2282} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:59] {2466} INFO -  at 5.2s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:57:59] {2282} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:57:59] {2466} INFO -  at 5.6s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:57:59] {2282} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:58:00] {2466} INFO -  at 6.5s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:58:00] {2282} INFO - iteration 25, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:58:00] {2466} INFO -  at 7.0s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:58:00] {2282} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:58:01] {2466} INFO -  at 7.8s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 12-19 16:58:01] {2282} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl.logger: 12-19 16:58:05] {2466} INFO -  at 12.0s,\testimator xgboost's best error=0.0130,\tbest estimator xgboost's best error=0.0130\n",
      "[flaml.automl.logger: 12-19 16:58:10] {2724} INFO - retrain xgboost for 4.4s\n",
      "[flaml.automl.logger: 12-19 16:58:10] {2727} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=np.float64(0.5402300960155632),\n",
      "              colsample_bynode=None,\n",
      "              colsample_bytree=np.float64(0.7167007041614387), device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, feature_weights=None,\n",
      "              gamma=None, grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None,\n",
      "              learning_rate=np.float64(0.14254591248827683), max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=376,\n",
      "              min_child_weight=np.float64(0.7371584010011801), missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=73,\n",
      "              n_jobs=-1, num_parallel_tree=None, ...)\n",
      "[flaml.automl.logger: 12-19 16:58:10] {2009} INFO - fit succeeded\n",
      "[flaml.automl.logger: 12-19 16:58:10] {2010} INFO - Time taken to find the best model: 12.007014036178589\n",
      "Best config: {'n_estimators': 73, 'max_leaves': 376, 'min_child_weight': np.float64(0.7371584010011801), 'learning_rate': np.float64(0.14254591248827683), 'subsample': np.float64(0.8171103690908121), 'colsample_bylevel': np.float64(0.5402300960155632), 'colsample_bytree': np.float64(0.7167007041614387), 'reg_alpha': np.float64(0.0645071425428955), 'reg_lambda': np.float64(4.989030667988049)}\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "automl = AutoML()\n",
    "\n",
    "\n",
    "automl_settings = {\n",
    "    \"time_budget\": 15,  # seconds\n",
    "    # \"train_time_limit\": 1,  # seconds\n",
    "    \"task\": \"classification\",\n",
    "    \"metric\": \"log_loss\",\n",
    "    \"estimator_list\": [\"xgboost\"],\n",
    "    # \"split_type\": time_series_split,\n",
    "    \"early_stop\": True,\n",
    "}\n",
    "\n",
    "\n",
    "X_train, y_train = prep_X_y(df_train, \"tantrum_within_60m\")\n",
    "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "print(\"Best config:\", automl.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ad07d",
   "metadata": {},
   "source": [
    "-   Task = predict tantrum within 60m\n",
    "-   Test = 15m, 30m, 60m period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1b87da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for prediction frequency: 15m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5220\n",
      "XGB ROC-AUC: 0.6587\n",
      "\n",
      "Evaluating for prediction frequency: 30m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5268\n",
      "XGB ROC-AUC: 0.6513\n",
      "\n",
      "Evaluating for prediction frequency: 60m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5148\n",
      "XGB ROC-AUC: 0.6310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "positive_model = DummyClassifier(strategy=\"constant\", constant=1).fit(X_train, y_train)\n",
    "hr_model = y_hat_hr = HrModel()\n",
    "model = automl\n",
    "\n",
    "df_test_60m = df_test[df_test[\"ActivityDateTime\"].dt.minute.isin([0])]\n",
    "df_test_30m = df_test[df_test[\"ActivityDateTime\"].dt.minute.isin([0, 30])]\n",
    "\n",
    "for label, df in [\n",
    "    (\"15m\", df_test),\n",
    "    (\"30m\", df_test_30m),\n",
    "    (\"60m\", df_test_60m),\n",
    "]:\n",
    "    print(f\"Evaluating for prediction frequency: {label}\")\n",
    "    X_test, y_test = prep_X_y(df, \"tantrum_within_60m\")\n",
    "\n",
    "    y_hat_positive = positive_model.predict(X_test)\n",
    "    print(\n",
    "        f\"Positive Model (Control) ROC-AUC: {roc_auc_score(y_test, y_hat_positive):.4f}\"\n",
    "    )\n",
    "\n",
    "    y_hat_hr = hr_model.predict_proba(X_test)\n",
    "    print(f\"HR Model ROC-AUC: {roc_auc_score(y_test, y_hat_hr[:, 1]):.4f}\")\n",
    "\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "    print(f\"XGB ROC-AUC: {roc_auc_score(y_test, y_hat[:, 1]):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95c9c4",
   "metadata": {},
   "source": [
    "Now, fix frequency at 15m and test 15m, 30m, 60m lookahead windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8d603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for lookahead window: 15m\n",
      "Total positive samples: 159 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5260\n",
      "XGB ROC-AUC: 0.6742\n",
      "\n",
      "Evaluating for lookahead window: 30m\n",
      "Total positive samples: 313 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5249\n",
      "XGB ROC-AUC: 0.6668\n",
      "\n",
      "Evaluating for lookahead window: 60m\n",
      "Total positive samples: 612 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5220\n",
      "XGB ROC-AUC: 0.6587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for window in [\"15m\", \"30m\", \"60m\"]:\n",
    "    automl = AutoML()\n",
    "    X_train, y_train = prep_X_y(df_train, f\"tantrum_within_{window}\")\n",
    "    automl.fit(X_train=X_train, y_train=y_train, verbose=False, **automl_settings)\n",
    "\n",
    "    print(f\"Evaluating for lookahead window: {window}\")\n",
    "    X_test, y_test = prep_X_y(df_test, response_column=f\"tantrum_within_{window}\")\n",
    "    print(f\"Total positive samples: {y_test.sum()} out of {len(y_test)}\")\n",
    "\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "    y_hat_positive = positive_model.predict(X_test)\n",
    "    y_hat_hr = hr_model.predict_proba(X_test)\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "\n",
    "    print(\n",
    "        f\"Positive Model (Control) ROC-AUC: {roc_auc_score(y_test, y_hat_positive):.4f}\"\n",
    "    )\n",
    "    print(f\"HR Model ROC-AUC: {roc_auc_score(y_test, y_hat_hr[:, 1]):.4f}\")\n",
    "    print(f\"XGB ROC-AUC: {roc_auc_score(y_test, y_hat[:, 1]):.4f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pistachio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
