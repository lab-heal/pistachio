{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e89cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/rygbfpzx3l7fcpbc7540q88w0000gr/T/ipykernel_28091/904874444.py:9: DtypeWarning: Columns (95,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import engineer_features, prep_X_y\n",
    "\n",
    "DATA_DIR = Path(\"./pistachio_1_data\")\n",
    "dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d373a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhu.henian/projects/pistachio/util.py:135: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(df[\"ActivityDateTime\"]) - pd.to_datetime(df[\"Therapy Start\"])\n"
     ]
    }
   ],
   "source": [
    "sorted_dyads_df = dyads_df.sort_values(\n",
    "    by=\"ActivityDateTime\", key=lambda x: pd.to_datetime(x)\n",
    ")\n",
    "cleaned_dyads_df = engineer_features(\n",
    "    sorted_dyads_df,\n",
    "    stress_lookback_days=0,\n",
    ")\n",
    "cleaned_dyads_df = pd.concat(list(cleaned_dyads_df.values()), axis=1)\n",
    "\n",
    "df_train = cleaned_dyads_df[cleaned_dyads_df[\"Arm_Sham\"]]\n",
    "df_test = cleaned_dyads_df[~cleaned_dyads_df[\"Arm_Sham\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e2bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 01-19 18:24:05] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 01-19 18:24:05] {1763} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 01-19 18:24:05] {1862} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl.logger: 01-19 18:24:05] {1979} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 0, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2417} INFO - Estimated sufficient time budget=4246s. Estimated necessary time budget=4s.\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.0262,\tbest estimator xgboost's best error=0.0262\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.0262,\tbest estimator xgboost's best error=0.0262\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.0262,\tbest estimator xgboost's best error=0.0262\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.0254,\tbest estimator xgboost's best error=0.0254\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.0254,\tbest estimator xgboost's best error=0.0254\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.0254,\tbest estimator xgboost's best error=0.0254\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 6, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.0254,\tbest estimator xgboost's best error=0.0254\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.0254,\tbest estimator xgboost's best error=0.0254\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.0239,\tbest estimator xgboost's best error=0.0239\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.0237,\tbest estimator xgboost's best error=0.0237\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.0237,\tbest estimator xgboost's best error=0.0237\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.0232,\tbest estimator xgboost's best error=0.0232\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.0229,\tbest estimator xgboost's best error=0.0229\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0229,\tbest estimator xgboost's best error=0.0229\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.0229,\tbest estimator xgboost's best error=0.0229\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.7s,\testimator xgboost's best error=0.0229,\tbest estimator xgboost's best error=0.0229\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2466} INFO -  at 1.8s,\testimator xgboost's best error=0.0229,\tbest estimator xgboost's best error=0.0229\n",
      "[flaml.automl.logger: 01-19 18:24:05] {2282} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2466} INFO -  at 1.9s,\testimator xgboost's best error=0.0210,\tbest estimator xgboost's best error=0.0210\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2282} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2466} INFO -  at 2.1s,\testimator xgboost's best error=0.0210,\tbest estimator xgboost's best error=0.0210\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2282} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2466} INFO -  at 2.3s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2282} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2466} INFO -  at 2.5s,\testimator xgboost's best error=0.0198,\tbest estimator xgboost's best error=0.0198\n",
      "[flaml.automl.logger: 01-19 18:24:06] {2282} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:07] {2466} INFO -  at 3.1s,\testimator xgboost's best error=0.0190,\tbest estimator xgboost's best error=0.0190\n",
      "[flaml.automl.logger: 01-19 18:24:07] {2282} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:08] {2466} INFO -  at 4.0s,\testimator xgboost's best error=0.0174,\tbest estimator xgboost's best error=0.0174\n",
      "[flaml.automl.logger: 01-19 18:24:08] {2282} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:08] {2466} INFO -  at 4.7s,\testimator xgboost's best error=0.0174,\tbest estimator xgboost's best error=0.0174\n",
      "[flaml.automl.logger: 01-19 18:24:08] {2282} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:09] {2466} INFO -  at 5.3s,\testimator xgboost's best error=0.0174,\tbest estimator xgboost's best error=0.0174\n",
      "[flaml.automl.logger: 01-19 18:24:09] {2282} INFO - iteration 25, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:10] {2466} INFO -  at 6.4s,\testimator xgboost's best error=0.0174,\tbest estimator xgboost's best error=0.0174\n",
      "[flaml.automl.logger: 01-19 18:24:10] {2282} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:10] {2466} INFO -  at 6.6s,\testimator xgboost's best error=0.0174,\tbest estimator xgboost's best error=0.0174\n",
      "[flaml.automl.logger: 01-19 18:24:10] {2282} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:12] {2466} INFO -  at 8.0s,\testimator xgboost's best error=0.0169,\tbest estimator xgboost's best error=0.0169\n",
      "[flaml.automl.logger: 01-19 18:24:12] {2282} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:14] {2466} INFO -  at 10.2s,\testimator xgboost's best error=0.0167,\tbest estimator xgboost's best error=0.0167\n",
      "[flaml.automl.logger: 01-19 18:24:14] {2282} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:15] {2466} INFO -  at 11.6s,\testimator xgboost's best error=0.0167,\tbest estimator xgboost's best error=0.0167\n",
      "[flaml.automl.logger: 01-19 18:24:15] {2282} INFO - iteration 30, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:16] {2466} INFO -  at 12.2s,\testimator xgboost's best error=0.0167,\tbest estimator xgboost's best error=0.0167\n",
      "[flaml.automl.logger: 01-19 18:24:16] {2282} INFO - iteration 31, current learner xgboost\n",
      "[flaml.automl.logger: 01-19 18:24:19] {2466} INFO -  at 14.9s,\testimator xgboost's best error=0.0167,\tbest estimator xgboost's best error=0.0167\n",
      "[flaml.automl.logger: 01-19 18:24:21] {2724} INFO - retrain xgboost for 2.5s\n",
      "[flaml.automl.logger: 01-19 18:24:21] {2727} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=np.float64(0.6794916722626781),\n",
      "              colsample_bynode=None,\n",
      "              colsample_bytree=np.float64(0.7746932016556806), device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, feature_weights=None,\n",
      "              gamma=None, grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None,\n",
      "              learning_rate=np.float64(0.07395672669232034), max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=167,\n",
      "              min_child_weight=np.float64(5.180034691127463), missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=108,\n",
      "              n_jobs=-1, num_parallel_tree=None, ...)\n",
      "[flaml.automl.logger: 01-19 18:24:21] {2009} INFO - fit succeeded\n",
      "[flaml.automl.logger: 01-19 18:24:21] {2010} INFO - Time taken to find the best model: 10.215046167373657\n",
      "Best config: {'n_estimators': 108, 'max_leaves': 167, 'min_child_weight': np.float64(5.180034691127463), 'learning_rate': np.float64(0.07395672669232034), 'subsample': np.float64(0.9487233160264894), 'colsample_bylevel': np.float64(0.6794916722626781), 'colsample_bytree': np.float64(0.7746932016556806), 'reg_alpha': np.float64(1.0334737782456975), 'reg_lambda': np.float64(1.1766759465359897)}\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "automl = AutoML()\n",
    "\n",
    "\n",
    "automl_settings = {\n",
    "    \"time_budget\": 15,  # seconds\n",
    "    # \"train_time_limit\": 1,  # seconds\n",
    "    \"task\": \"classification\",\n",
    "    \"metric\": \"log_loss\",\n",
    "    \"estimator_list\": [\"xgboost\"],\n",
    "    # \"split_type\": time_series_split,\n",
    "    \"early_stop\": True,\n",
    "}\n",
    "\n",
    "\n",
    "X_train, y_train = prep_X_y(df_train, \"tantrum_within_60m\")\n",
    "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "print(\"Best config:\", automl.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ad07d",
   "metadata": {},
   "source": [
    "-   Task = predict tantrum within 60m\n",
    "-   Test = 15m, 30m, 60m period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b87da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for prediction frequency: 15m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5560\n",
      "XGB ROC-AUC: 0.6915\n",
      "\n",
      "Evaluating for prediction frequency: 30m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5536\n",
      "XGB ROC-AUC: 0.6880\n",
      "\n",
      "Evaluating for prediction frequency: 60m\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5617\n",
      "XGB ROC-AUC: 0.6921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hr_model import HrModel\n",
    "\n",
    "positive_model = DummyClassifier(strategy=\"constant\", constant=1).fit(X_train, y_train)\n",
    "hr_model = y_hat_hr = HrModel()\n",
    "model = automl\n",
    "\n",
    "df_test_60m = df_test[df_test[\"ActivityDateTime\"].dt.minute.isin([0])]\n",
    "df_test_30m = df_test[df_test[\"ActivityDateTime\"].dt.minute.isin([0, 30])]\n",
    "\n",
    "for label, df in [\n",
    "    (\"15m\", df_test),\n",
    "    (\"30m\", df_test_30m),\n",
    "    (\"60m\", df_test_60m),\n",
    "]:\n",
    "    print(f\"Evaluating for prediction frequency: {label}\")\n",
    "    X_test, y_test = prep_X_y(df, \"tantrum_within_60m\")\n",
    "\n",
    "    y_hat_positive = positive_model.predict(X_test)\n",
    "    print(\n",
    "        f\"Positive Model (Control) ROC-AUC: {roc_auc_score(y_test, y_hat_positive):.4f}\"\n",
    "    )\n",
    "\n",
    "    y_hat_hr = hr_model.predict_proba(X_test)\n",
    "    print(f\"HR Model ROC-AUC: {roc_auc_score(y_test, y_hat_hr[:, 1]):.4f}\")\n",
    "\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "    print(f\"XGB ROC-AUC: {roc_auc_score(y_test, y_hat[:, 1]):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95c9c4",
   "metadata": {},
   "source": [
    "Now, fix frequency at 15m and test 15m, 30m, 60m lookahead windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8d603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for lookahead window: 15m\n",
      "Total positive samples: 159 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5260\n",
      "XGB ROC-AUC: 0.6901\n",
      "\n",
      "Evaluating for lookahead window: 30m\n",
      "Total positive samples: 313 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5249\n",
      "XGB ROC-AUC: 0.6954\n",
      "\n",
      "Evaluating for lookahead window: 60m\n",
      "Total positive samples: 612 out of 255981\n",
      "Positive Model (Control) ROC-AUC: 0.5000\n",
      "HR Model ROC-AUC: 0.5220\n",
      "XGB ROC-AUC: 0.6915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for window in [\"15m\", \"30m\", \"60m\"]:\n",
    "    automl = AutoML()\n",
    "    X_train, y_train = prep_X_y(df_train, f\"tantrum_within_{window}\")\n",
    "    automl.fit(X_train=X_train, y_train=y_train, verbose=False, **automl_settings)\n",
    "\n",
    "    print(f\"Evaluating for lookahead window: {window}\")\n",
    "    X_test, y_test = prep_X_y(df_test, response_column=f\"tantrum_within_{window}\")\n",
    "    print(f\"Total positive samples: {y_test.sum()} out of {len(y_test)}\")\n",
    "\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "    y_hat_positive = positive_model.predict(X_test)\n",
    "    y_hat_hr = hr_model.predict_proba(X_test)\n",
    "    y_hat = model.predict_proba(X_test)\n",
    "\n",
    "    print(\n",
    "        f\"Positive Model (Control) ROC-AUC: {roc_auc_score(y_test, y_hat_positive):.4f}\"\n",
    "    )\n",
    "    print(f\"HR Model ROC-AUC: {roc_auc_score(y_test, y_hat_hr[:, 1]):.4f}\")\n",
    "    print(f\"XGB ROC-AUC: {roc_auc_score(y_test, y_hat[:, 1]):.4f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pistachio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
