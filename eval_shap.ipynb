{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef864836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import shap\n",
    "from flaml import AutoML\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from util import engineer_features, prep_X_y\n",
    "\n",
    "DATA_DIR = Path(\"./pistachio_1_data\")\n",
    "dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")\n",
    "\n",
    "sorted_dyads_df = dyads_df.sort_values(\n",
    "    by=\"ActivityDateTime\", key=lambda x: pd.to_datetime(x)\n",
    ")\n",
    "cleaned_dyads_dfs = engineer_features(\n",
    "    sorted_dyads_df,\n",
    "    stress_lookback_days=0,\n",
    "    sleep_days_to_keep=[1, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from modeling import feature_supersets, supersets_to_test\n",
    "\n",
    "week_range = (0, 15)\n",
    "active_hours = (7, 20)\n",
    "\n",
    "\n",
    "def bootstrap(df: pd.DataFrame, n_samples: int) -> pd.DataFrame:\n",
    "    boot_df = pd.DataFrame()\n",
    "    for _ in range(n_samples):\n",
    "        boot_df = pd.concat(\n",
    "            [\n",
    "                boot_df,\n",
    "                df.sample(frac=1, replace=True, random_state=None),\n",
    "            ]\n",
    "        )\n",
    "    return boot_df\n",
    "\n",
    "\n",
    "def population_model_shap(supersets: list[str]) -> shap.Explanation:\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            cleaned_dyads_dfs[\"index\"],\n",
    "            cleaned_dyads_dfs[\"response\"],\n",
    "        ]\n",
    "        + [\n",
    "            cleaned_dyads_dfs[fs]\n",
    "            for superset in supersets\n",
    "            for fs in feature_supersets[superset]\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    df = df[df[\"therapy_week\"].between(week_range[0], week_range[1])]\n",
    "    df = df[df[\"ActivityDateTime\"].dt.hour.between(active_hours[0], active_hours[1])]\n",
    "\n",
    "    df_train = df[df[\"Arm_Sham\"]]\n",
    "    df_test = df[~df[\"Arm_Sham\"]]\n",
    "\n",
    "    groups = df_train[\"dyad\"]\n",
    "    automl_settings = {\n",
    "        \"max_iter\": 100,\n",
    "        \"estimator_list\": [\"xgboost\"],\n",
    "        \"early_stop\": True,\n",
    "        \"eval_method\": \"cv\",\n",
    "        \"split_type\": \"group\",\n",
    "        \"groups\": groups,\n",
    "        \"retrain_full\": False,\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "\n",
    "    window = \"60m\"\n",
    "    X_train, y_train = prep_X_y(df_train, f\"tantrum_within_{window}\")\n",
    "    X_test, y_test = prep_X_y(df_test, response_column=f\"tantrum_within_{window}\")\n",
    "\n",
    "    automl = AutoML()\n",
    "    automl.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        **automl_settings,\n",
    "    )\n",
    "    model = XGBClassifier(**automl.best_config, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    # print(\"Best config:\", automl.best_config)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # Compute ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    # print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def individual_model_shap(supersets: list[str], week: int) -> shap.Explanation:\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            cleaned_dyads_dfs[\"index\"],\n",
    "            cleaned_dyads_dfs[\"response\"],\n",
    "        ]\n",
    "        + [\n",
    "            cleaned_dyads_dfs[fs]\n",
    "            for superset in supersets\n",
    "            for fs in feature_supersets[superset]\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    df = df[df[\"therapy_week\"].between(week_range[0], week_range[1])]\n",
    "    df = df[df[\"ActivityDateTime\"].dt.hour.between(active_hours[0], active_hours[1])]\n",
    "\n",
    "    df_sham = df[df[\"Arm_Sham\"]]\n",
    "    df_active = df[~df[\"Arm_Sham\"]]\n",
    "\n",
    "    automl = AutoML()\n",
    "    automl_settings = {\n",
    "        \"max_iter\": 100,\n",
    "        \"estimator_list\": [\"xgboost\"],\n",
    "        \"eval_method\": \"cv\",\n",
    "        \"split_type\": \"group\",\n",
    "        \"groups\": df_sham[\"dyad\"],\n",
    "        \"verbose\": 0,\n",
    "        \"retrain_full\": False,\n",
    "    }\n",
    "    X_train_init, y_train_init = prep_X_y(df_sham, \"tantrum_within_60m\")\n",
    "    automl.fit(\n",
    "        X_train=X_train_init,\n",
    "        y_train=y_train_init,\n",
    "        **automl_settings,\n",
    "    )\n",
    "    # print(\"Best config:\", automl.best_config)\n",
    "\n",
    "    all_shaps = []\n",
    "    all_features = []\n",
    "    base_values = []\n",
    "\n",
    "    all_proba = []\n",
    "    all_trues = []\n",
    "\n",
    "    for dyad, dyad_df in df_active.groupby(\"dyad\"):\n",
    "        add_df = dyad_df[dyad_df[\"therapy_week\"] < week]\n",
    "        add_df = bootstrap(add_df, df_active[\"dyad\"].nunique())\n",
    "\n",
    "        df_train = pd.concat([df_sham, add_df])\n",
    "        df_test = dyad_df[dyad_df[\"therapy_week\"] == week]\n",
    "\n",
    "        window = \"60m\"\n",
    "        X_train, y_train = prep_X_y(df_train, f\"tantrum_within_{window}\")\n",
    "        X_test, y_test = prep_X_y(df_test, response_column=f\"tantrum_within_{window}\")\n",
    "\n",
    "        # Skip if only one class present or no data past that week\n",
    "        if not len(df_test) or y_test.nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        model = XGBClassifier(**automl.best_config, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # # Compute ROC AUC\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        all_proba.extend(y_pred_proba)\n",
    "        all_trues.extend(y_test)\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.Explainer(model)\n",
    "        sv = explainer(X_test)\n",
    "        all_shaps.append(sv.values)  # The SHAP values\n",
    "        all_features.append(sv.data)  # The actual feature values\n",
    "        base_values.append(sv.base_values)  # The starting point (expected value)\n",
    "\n",
    "    # aggregated_shaps = np.vstack(all_shaps)\n",
    "    # aggregated_features = np.vstack(all_features)\n",
    "    # aggregated_base = np.concatenate(base_values)\n",
    "\n",
    "    auroc = roc_auc_score(all_trues, all_proba)\n",
    "    # print(f\"AUROC: {auroc}\")\n",
    "\n",
    "    aggregated_shaps = np.vstack(all_shaps)\n",
    "    aggregated_features = np.vstack(all_features)\n",
    "    aggregated_base = np.concatenate(base_values)\n",
    "\n",
    "    exp = shap.Explanation(\n",
    "        values=aggregated_shaps,\n",
    "        data=aggregated_features,\n",
    "        base_values=aggregated_base,\n",
    "        feature_names=X_train_init.columns.tolist(),\n",
    "    )\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for superset in supersets_to_test:\n",
    "    print(superset)\n",
    "    for week in (0, 7, 14):\n",
    "        print(week)\n",
    "        explanation = individual_model_shap(superset, week=week)\n",
    "\n",
    "        mean = np.nanmean(explanation.values)\n",
    "        std = np.nanstd(explanation.values)\n",
    "        mask = np.abs(explanation.values - mean) <= 2 * std\n",
    "        filtered_values = np.where(mask, explanation.values, np.nan)\n",
    "        explanation.values = filtered_values\n",
    "\n",
    "        # shap.plots.bar(explanation, max_display=15)\n",
    "        shap.plots.beeswarm(explanation, group_remaining_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6fce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pistachio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
