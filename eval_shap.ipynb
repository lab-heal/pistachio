{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef864836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/rygbfpzx3l7fcpbc7540q88w0000gr/T/ipykernel_18645/1342709441.py:12: DtypeWarning: Columns (131,132,133) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")\n",
      "/Users/zhu.henian/projects/pistachio/util.py:150: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(df[\"ActivityDateTime\"]) - pd.to_datetime(df[\"Therapy Start\"])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "\n",
    "from util import engineer_features, prep_X_y\n",
    "\n",
    "DATA_DIR = Path(\"./pistachio_1_data\")\n",
    "dyads_df = pd.read_csv(DATA_DIR / \"all_dyads.csv\")\n",
    "\n",
    "sorted_dyads_df = dyads_df.sort_values(\n",
    "    by=\"ActivityDateTime\", key=lambda x: pd.to_datetime(x)\n",
    ")\n",
    "cleaned_dyads_dfs = engineer_features(\n",
    "    sorted_dyads_df,\n",
    "    stress_lookback_days=0,\n",
    "    sleep_days_to_keep=[1, 2],\n",
    ")\n",
    "\n",
    "feature_sets = {\n",
    "    \"index\": True,\n",
    "    \"response\": True,\n",
    "    #\n",
    "    \"hr\": True,\n",
    "    \"activity\": True,\n",
    "    \"sleep\": True,\n",
    "    \"stress\": True,\n",
    "    \"overnight_hrv\": True,\n",
    "    \"medical\": True,\n",
    "    \"therapy\": True,\n",
    "    \"child_demo\": True,\n",
    "    \"parent_demo\": True,\n",
    "    \"temporal\": False,\n",
    "}\n",
    "\n",
    "df = pd.concat(\n",
    "    [cleaned_dyads_dfs[key] for key, include in feature_sets.items() if include], axis=1\n",
    ")\n",
    "df_sham = df[df[\"Arm_Sham\"]]\n",
    "df_treat = df[~df[\"Arm_Sham\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4544e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_sham\n",
    "df_test = df_treat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921855f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, PredefinedSplit\n",
    "\n",
    "dyad_labels = df_train[\"dyad\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds = np.zeros(len(df_train), dtype=int)\n",
    "for fold_idx, (_, val_idx) in enumerate(\n",
    "    kf.split(np.zeros(len(dyad_labels)), dyad_labels)\n",
    "):\n",
    "    folds[val_idx] = fold_idx\n",
    "cv = PredefinedSplit(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e0e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 01-27 12:34:04] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 01-27 12:34:04] {1763} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 01-27 12:34:04] {1862} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 01-27 12:34:04] {1979} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 0, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2417} INFO - Estimated sufficient time budget=3683s. Estimated necessary time budget=4s.\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 0.9s,\testimator xgboost's best error=0.3206,\tbest estimator xgboost's best error=0.3206\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 1.0s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 1.0s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 1.0s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 1.0s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2466} INFO -  at 1.0s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:04] {2282} INFO - iteration 6, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.1s,\testimator xgboost's best error=0.2880,\tbest estimator xgboost's best error=0.2880\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.1s,\testimator xgboost's best error=0.2877,\tbest estimator xgboost's best error=0.2877\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.2505,\tbest estimator xgboost's best error=0.2505\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.2359,\tbest estimator xgboost's best error=0.2359\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.2359,\tbest estimator xgboost's best error=0.2359\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.2359,\tbest estimator xgboost's best error=0.2359\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.2359,\tbest estimator xgboost's best error=0.2359\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.2358,\tbest estimator xgboost's best error=0.2358\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.2358,\tbest estimator xgboost's best error=0.2358\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.4s,\testimator xgboost's best error=0.2358,\tbest estimator xgboost's best error=0.2358\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.2358,\tbest estimator xgboost's best error=0.2358\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.1909,\tbest estimator xgboost's best error=0.1909\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.6s,\testimator xgboost's best error=0.1909,\tbest estimator xgboost's best error=0.1909\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.8s,\testimator xgboost's best error=0.1513,\tbest estimator xgboost's best error=0.1513\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2466} INFO -  at 1.9s,\testimator xgboost's best error=0.1513,\tbest estimator xgboost's best error=0.1513\n",
      "[flaml.automl.logger: 01-27 12:34:05] {2282} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:06] {2466} INFO -  at 2.6s,\testimator xgboost's best error=0.1455,\tbest estimator xgboost's best error=0.1455\n",
      "[flaml.automl.logger: 01-27 12:34:06] {2282} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:07] {2466} INFO -  at 3.6s,\testimator xgboost's best error=0.1019,\tbest estimator xgboost's best error=0.1019\n",
      "[flaml.automl.logger: 01-27 12:34:07] {2282} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:08] {2466} INFO -  at 4.3s,\testimator xgboost's best error=0.1019,\tbest estimator xgboost's best error=0.1019\n",
      "[flaml.automl.logger: 01-27 12:34:08] {2282} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:08] {2466} INFO -  at 4.7s,\testimator xgboost's best error=0.1019,\tbest estimator xgboost's best error=0.1019\n",
      "[flaml.automl.logger: 01-27 12:34:08] {2282} INFO - iteration 25, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:10] {2466} INFO -  at 6.1s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:10] {2282} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:10] {2466} INFO -  at 6.2s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:10] {2282} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:11] {2466} INFO -  at 8.0s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:11] {2282} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:12] {2466} INFO -  at 8.7s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:12] {2282} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:13] {2466} INFO -  at 9.4s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:13] {2282} INFO - iteration 30, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:13] {2466} INFO -  at 9.7s,\testimator xgboost's best error=0.1004,\tbest estimator xgboost's best error=0.1004\n",
      "[flaml.automl.logger: 01-27 12:34:13] {2282} INFO - iteration 31, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:15] {2466} INFO -  at 11.7s,\testimator xgboost's best error=0.0980,\tbest estimator xgboost's best error=0.0980\n",
      "[flaml.automl.logger: 01-27 12:34:15] {2282} INFO - iteration 32, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:17] {2466} INFO -  at 13.7s,\testimator xgboost's best error=0.0980,\tbest estimator xgboost's best error=0.0980\n",
      "[flaml.automl.logger: 01-27 12:34:17] {2282} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:19] {2466} INFO -  at 15.6s,\testimator xgboost's best error=0.0980,\tbest estimator xgboost's best error=0.0980\n",
      "[flaml.automl.logger: 01-27 12:34:19] {2282} INFO - iteration 34, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:21] {2466} INFO -  at 17.2s,\testimator xgboost's best error=0.0980,\tbest estimator xgboost's best error=0.0980\n",
      "[flaml.automl.logger: 01-27 12:34:21] {2282} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:22] {2466} INFO -  at 18.7s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:22] {2282} INFO - iteration 36, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:23] {2466} INFO -  at 20.0s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:23] {2282} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:24] {2466} INFO -  at 20.7s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:24] {2282} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:25] {2466} INFO -  at 21.7s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:25] {2282} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:26] {2466} INFO -  at 22.7s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:26] {2282} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:27] {2466} INFO -  at 23.5s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:27] {2282} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:28] {2466} INFO -  at 24.2s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:28] {2282} INFO - iteration 42, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:28] {2466} INFO -  at 24.9s,\testimator xgboost's best error=0.0878,\tbest estimator xgboost's best error=0.0878\n",
      "[flaml.automl.logger: 01-27 12:34:28] {2282} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:30] {2466} INFO -  at 26.8s,\testimator xgboost's best error=0.0388,\tbest estimator xgboost's best error=0.0388\n",
      "[flaml.automl.logger: 01-27 12:34:30] {2282} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl.logger: 01-27 12:34:32] {2466} INFO -  at 28.7s,\testimator xgboost's best error=0.0388,\tbest estimator xgboost's best error=0.0388\n",
      "[flaml.automl.logger: 01-27 12:34:32] {2589} INFO - selected model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=np.float64(0.4551389346892553),\n",
      "              colsample_bynode=None,\n",
      "              colsample_bytree=np.float64(0.8152476333803775), device=None,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, feature_weights=None,\n",
      "              gamma=None, grow_policy='lossguide', importance_type=None,\n",
      "              interaction_constraints=None,\n",
      "              learning_rate=np.float64(0.032883435998917514), max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=40,\n",
      "              min_child_weight=np.float64(1.4580801851439538), missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=517,\n",
      "              n_jobs=-1, num_parallel_tree=None, ...)\n",
      "[flaml.automl.logger: 01-27 12:34:32] {2009} INFO - fit succeeded\n",
      "[flaml.automl.logger: 01-27 12:34:32] {2010} INFO - Time taken to find the best model: 26.813788890838623\n",
      "\u001b[33m[flaml.automl.logger: 01-27 12:34:32] {2020} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\u001b[0m\n",
      "Best config: {'n_estimators': 517, 'max_leaves': 40, 'min_child_weight': np.float64(1.4580801851439538), 'learning_rate': np.float64(0.032883435998917514), 'subsample': np.float64(0.7936710242493861), 'colsample_bylevel': np.float64(0.4551389346892553), 'colsample_bytree': np.float64(0.8152476333803775), 'reg_alpha': np.float64(0.011058294857426279), 'reg_lambda': np.float64(0.0013471156840415867)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "automl = AutoML()\n",
    "\n",
    "groups = df_train[\"dyad\"]\n",
    "automl_settings = {\n",
    "    \"time_budget\": 30,  # seconds\n",
    "    \"train_time_limit\": 2,  # seconds\n",
    "    \"pred_time_limit\": 1e-4,  # seconds\n",
    "    \"task\": \"classification\",\n",
    "    # \"metric\": \"log_loss\",\n",
    "    \"estimator_list\": [\"xgboost\"],\n",
    "    \"early_stop\": True,\n",
    "    \"split_type\": cv,\n",
    "    \"retrain_full\": False,\n",
    "}\n",
    "\n",
    "\n",
    "X_train, y_train = prep_X_y(df_train, \"tantrum_within_60m\")\n",
    "automl.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    **automl_settings,\n",
    ")\n",
    "print(\"Best config:\", automl.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6bfc509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6637\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Fit the model if not already fitted\n",
    "\n",
    "window = \"60m\"\n",
    "X_train, y_train = prep_X_y(df_train, f\"tantrum_within_{window}\")\n",
    "X_test, y_test = prep_X_y(df_test, response_column=f\"tantrum_within_{window}\")\n",
    "model = XGBClassifier(**automl.best_config)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb110848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "# Note: Bar plot does not accept \"group_remaining_features\" argument\n",
    "shap.plots.bar(shap_values, max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e538636",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=15, group_remaining_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"hr_moving_avg_10m\"], color=shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pistachio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
